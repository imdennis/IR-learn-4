{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from numba import jit\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "root_dir = './'\n",
    "doc_dir = './Document/'\n",
    "query_dir = './Query/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_names = []\n",
    "query_names = []\n",
    "\n",
    "with open('doc_list.txt', 'r') as f:\n",
    "    doc_names = [line.strip() for line in f]\n",
    "\n",
    "with open('query_list.txt', 'r') as f:\n",
    "    query_names = [line.strip() for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = {}\n",
    "\n",
    "for i in range(len(doc_names)):\n",
    "    with open(doc_dir + doc_names[i] , 'r') as f:\n",
    "        next(f)\n",
    "        next(f)\n",
    "        next(f)\n",
    "        words = []\n",
    "        for line in f:\n",
    "            words+=line.rstrip()[:-3].split(' ')\n",
    "            \n",
    "        training_set[i]=words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_set = {}\n",
    "for i in range(len(query_names)):\n",
    "    with open(query_dir + query_names[i] , 'r') as f:\n",
    "        words = []\n",
    "        for line in f:\n",
    "             words+=line.rstrip()[:-3].split(' ')\n",
    "        query_set[i] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_set)\n",
    "len(query_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_dj_wi = {}\n",
    "idf_wi = {}\n",
    "vc_dj = {}\n",
    "word_list = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf\n",
    "for j in range(len(training_set)):\n",
    "    tf_dj_wi[j] = {}\n",
    "    vc_dj[j] = vc = pd.value_counts(training_set[j])\n",
    "    \n",
    "    vc_index = list(vc.index)\n",
    "    for i in range(len(vc_index)):\n",
    "        tf_dj_wi[j][vc_index[i]] = vc[i] / len(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_list\n",
    "word_list = {}\n",
    "all_index = []\n",
    "for j in range(len(vc_dj)):\n",
    "    all_index += list(vc_dj[j].index)\n",
    "    \n",
    "w = np.unique(all_index)\n",
    "for i in range(len(w)):\n",
    "    word_list[str(w[i])] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13290"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#idf\n",
    "idf_wi = {}\n",
    "for j in range(len(training_set)):\n",
    "    vc = vc_dj[j]\n",
    "    vc_index = list(vc.index)\n",
    "    \n",
    "    for i in range(len(vc)):\n",
    "        if vc_index[i] not in idf_wi:\n",
    "            idf_wi[vc_index[i]] = vc[i]\n",
    "        else:\n",
    "            idf_wi[vc_index[i]] = idf_wi[vc_index[i]] + vc[i]\n",
    "\n",
    "len_docs = len(training_set)\n",
    "for word in idf_wi:\n",
    "    idf_wi[word] = math.log10(len_docs/idf_wi[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.400825696909526"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_wi['516']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf(query_set)\n",
    "def calc_tf_q(query_set):\n",
    "    new_tf_qj_wi = {}\n",
    "    for j in range(len(query_set)):\n",
    "        new_tf_qj_wi[j] = {}\n",
    "        query_vc = pd.value_counts(query_set[j])\n",
    "        vc_index = list(query_vc.index)\n",
    "        \n",
    "        for i in range(len(vc_index)):\n",
    "            new_tf_qj_wi[j][vc_index[i]] = query_vc[i]\n",
    "\n",
    "    return new_tf_qj_wi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.04 s, sys: 43.8 ms, total: 3.08 s\n",
      "Wall time: 3.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_qj_wi = calc_tf_q(query_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand(arr):\n",
    "    d = [0]*len(word_list)\n",
    "    wlist = list(word_list)\n",
    "    \n",
    "    for word in arr:\n",
    "        d[word_list[word]] = arr[word] \n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = {}\n",
    "norms = {}\n",
    "def vector_norm(tf):\n",
    "    s_tf = str(tf)\n",
    "    if s_tf not in vectors and s_tf not in norms:\n",
    "        vectors[s_tf] = tf2vector(tf)\n",
    "        norms[s_tf] = math.sqrt(sum(np.power(vectors[s_tf],2)))\n",
    "        return vectors[s_tf], norms[s_tf]\n",
    "    else:\n",
    "        return vectors[s_tf], norms[s_tf]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dj = [[0]*len(word_list)]*len(training_set)\n",
    "for j in range(len(training_set)):\n",
    "    tfidf_dj[j] = tf2vector(tf_dj_wi[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clac similarity of query k\n",
    "def sim(qk):\n",
    "\n",
    "    vector_q,norm_q = vector_norm(tf_qj_wi[qk])\n",
    "    sim_dj = cosine_similarity(vector_q, tfidf_dj)\n",
    "\n",
    "    return sortDocs(sim_dj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rele_non_rele(arr):\n",
    "    rele = arr[0:3]\n",
    "    non_rele = arr[len(arr)-50:len(arr)-1]\n",
    "    return rele, non_rele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sort documents\n",
    "def sortDocs(sim_dj):\n",
    "    ls = [ doc_names.index(name) for _,name in sorted(zip(sim_dj,doc_names))]\n",
    "    print(len(ls))\n",
    "    return reversed(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf2vector(tf):\n",
    "    vector = [0]*len(word_list)\n",
    "    for word in tf:\n",
    "        if word in  word_list: #some word might not in word list of documents\n",
    "            #vector = tf*idf \n",
    "            vector[word_list[word]] = tf[word]*idf_wi[word] \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calc new query\n",
    "def new_query(qi):\n",
    "#     print('new query',time.ctime())\n",
    "    \n",
    "    alpha = 0.3\n",
    "    beta = 1\n",
    "    gamma = 0\n",
    "\n",
    "    vector_q = tf2vector(tf_qj_wi[qi])\n",
    "    \n",
    "#     print('start sim',time.ctime())\n",
    "    rel, non_rel = rele_non_rele(sim(qi))\n",
    "#     print('rel_non_rel sim done',time.ctime())\n",
    "\n",
    "    \n",
    "    \n",
    "    sum_rel = [0]*len(word_list)\n",
    "    sum_non_rel = [0]*len(word_list)\n",
    "    \n",
    "    \n",
    "    #calc rel, non_rel together\n",
    "#     print('start combine rel non_rel',time.ctime())\n",
    "\n",
    "    for j in range(len(rel)):\n",
    "        rel_vector_d = tf2vector(tf_dj_wi[rel[j]])\n",
    "    for j in range(len(non_rel)):\n",
    "        non_rel_vector_d = tf2vector(tf_dj_wi[non_rel[j]])\n",
    "        \n",
    "        sum_rel = np.add(sum_rel,rel_vector_d)\n",
    "        sum_non_rel = np.add(sum_non_rel, non_rel_vector_d)\n",
    "        \n",
    "#     print('combine rel non_rel done',time.ctime())\n",
    "\n",
    "    new_q = np.multiply(vector_q, alpha) + np.multiply(sum_rel, beta) - np.multiply(sum_non_rel, gamma/50)\n",
    "#     print('complete parameters setting', time.ctime())\n",
    "    return new_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90369611]]\n",
      "0.9036961141150639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dennis/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/Users/Dennis/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "a1 = [2,1,3,4]\n",
    "a2 = [11,0,12,13]\n",
    "# print(np.add(a1,a2)[3])\n",
    "# np.power(a2,2)\n",
    "# str(a1)\n",
    "print(cosine_similarity(a1, a2))\n",
    "\n",
    "dot = np.dot(a1,a2)\n",
    "norm1 = math.sqrt(sum([math.pow(a,2) for a in a1]))\n",
    "norm2 = math.sqrt(sum([math.pow(a,2) for a in a2]))\n",
    "sim = dot/ norm1/ norm2\n",
    "print(sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dennis/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list_reverseiterator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-244-effb42215905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnew_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#array to dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-240-9fe44e880973>\u001b[0m in \u001b[0;36mnew_query\u001b[0;34m(qi)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#     print('start sim',time.ctime())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_rel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrele_non_rele\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#     print('rel_non_rel sim done',time.ctime())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-231-0f2815df6397>\u001b[0m in \u001b[0;36mrele_non_rele\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrele_non_rele\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrele\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnon_rele\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrele\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_rele\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list_reverseiterator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# calc new query\n",
    "\n",
    "# print submission title\n",
    "\n",
    "time_str = time.ctime()\n",
    "with open(time_str+'submission.txt', 'w') as t:\n",
    "    t.write('Query,RetrievedDocuments')\n",
    "\n",
    "for i in range(len(query_set)):\n",
    "    \n",
    "    with open(time_str+'submission.txt', 'a') as t:\n",
    "        t.write('\\n')\n",
    "\n",
    "    new_q = new_query(i)\n",
    "    \n",
    "    #array to dictionary\n",
    "    tf_q = {}\n",
    "    wl = list(word_list)\n",
    "    for w in range(len(wl)):\n",
    "        tf_q[wl[w]] = new_q[w]\n",
    "    \n",
    "    print(time.ctime(),'done q=',i)\n",
    "    \n",
    "    #sim\n",
    "    vector_q,norm_q = vector_norm(tf_q)\n",
    "    sim_dj = cosine_similarity(vector_q,tfidf_dj)\n",
    "    \n",
    "\n",
    "    docs = [name[i] for i in sortDocs(sim_dj)]\n",
    "    \n",
    "    with open(time_str+'submission.txt', 'a') as t:\n",
    "        t.write('%s,' % query_names[i])\n",
    "\n",
    "        for doc in docs:\n",
    "            t.write('%s ' % doc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 12, 11, 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = [2,1,3,4]\n",
    "a2 = [11,0,12,13]\n",
    "\n",
    "[x for _,x in reversed(sorted(zip(a1,a2)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
